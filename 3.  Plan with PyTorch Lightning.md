# Detailed Machine Learning Lesson Plan with PyTorch Lightning

## Objectives

- Set up a machine learning environment and install necessary libraries.
- Load and explore a churn modeling dataset.
- Preprocess data using encoding and scaling techniques.
- Build and train a neural network using PyTorch Lightning.
- Evaluate model performance using metrics and visualizations.

## Lesson Outline

### 1. Library Setup

**Objective:** Install and set up essential libraries within a virtual environment.

**Steps:**

1. **Create a Virtual Environment:**
   ```bash
   python -m venv ml_env
   ```

2. **Activate the Virtual Environment:**
   - On Windows:
     ```bash
     ml_env\Scripts\activate
     ```
   - On macOS/Linux:
     ```bash
     source ml_env/bin/activate
     ```

3. **Install Necessary Libraries:**
   ```bash
   pip install scikit-learn torch torchvision torchmetrics pandas seaborn matplotlib
   ```

### 2. Dataset Overview

**Objective:** Understand the churn modeling dataset.

**Details:**

- **Churn Modeling Dataset:** Contains information about bank customers, including features like credit score, age, tenure, and churn status.

### 3. Data Import and Initial Exploration

**Objective:** Load and explore the dataset.

**Steps:**

1. **Import Libraries:**
   ```python
   import pandas as pd
   import seaborn as sns
   import matplotlib.pyplot as plt
   ```

2. **Read Data:**
   ```python
   df = pd.read_csv('churn_data.csv')
   ```

3. **Initial Data Inspection:**
   ```python
   print(df.head())
   print(df.info())
   ```

### 4. Data Cleaning

**Objective:** Clean the dataset for analysis.

**Steps:**

1. **Drop Irrelevant Columns:**
   ```python
   df.drop(columns=['RowNumber', 'CustomerId', 'Surname'], inplace=True)
   ```

2. **Handle Missing Values:**
   ```python
   df.dropna(inplace=True)
   ```

3. **Remove Duplicates:**
   ```python
   df.drop_duplicates(inplace=True)
   ```

### 5. Data Exploration

**Objective:** Explore the dataset to understand its structure and distribution.

**Steps:**

1. **Check Data Types:**
   ```python
   print(df.dtypes)
   ```

2. **Class Imbalance:**
   - Check the distribution of the target variable (churn) to understand class imbalance.

3. **Visualization:**
   ```python
   sns.countplot(x='Exited', data=df)
   plt.title('Churn Distribution')
   plt.show()
   ```

### 6. Creating a LightningDataModule

**Objective:** Manage and prepare data using PyTorch Lightning.

**Steps:**

1. **Initialization (`__init__` Method):**
   ```python
   from pytorch_lightning import LightningDataModule
   from torch.utils.data import DataLoader, TensorDataset
   import torch

   class ChurnDataModule(LightningDataModule):
       def __init__(self, batch_size: int = 32):
           super().__init__()
           self.batch_size = batch_size
   ```

2. **Prepare Data (`prepare_data` Method):**
   ```python
   def prepare_data(self):
       self.df = pd.read_csv('churn_data.csv')
       self.df.drop(columns=['RowNumber', 'CustomerId', 'Surname'], inplace=True)
       self.df.dropna(inplace=True)
       self.df.drop_duplicates(inplace=True)
   ```

3. **Setup (`setup` Method):**
   ```python
   def setup(self, stage: str):
       X = self.df.drop('Exited', axis=1)
       y = self.df['Exited']

       X = pd.get_dummies(X, drop_first=True)
       X = (X - X.mean()) / X.std()

       self.dataset = TensorDataset(torch.tensor(X.values, dtype=torch.float32),
                                    torch.tensor(y.values, dtype=torch.float32))
   ```

4. **Data Loaders (`train_dataloader` and `val_dataloader` Methods):**
   ```python
   def train_dataloader(self):
       return DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True)

   def val_dataloader(self):
       return DataLoader(self.dataset, batch_size=self.batch_size)
   ```

### 7. Instantiating the Model and Data Module

**Objective:** Set up the model and data module for training.

**Steps:**

1. **Instantiate the Model:**
   ```python
   from pytorch_lightning import LightningModule
   import torch.nn as nn
   import torch.optim as optim

   class ChurnModel(LightningModule):
       def __init__(self, input_dim: int):
           super(ChurnModel, self).__init__()
           self.layer_1 = nn.Linear(input_dim, 16)
           self.layer_2 = nn.Linear(16, 8)
           self.output = nn.Linear(8, 1)
           self.criterion = nn.BCEWithLogitsLoss()

       def forward(self, x):
           x = torch.relu(self.layer_1(x))
           x = torch.relu(self.layer_2(x))
           return torch.sigmoid(self.output(x))

       def training_step(self, batch, batch_idx):
           x, y = batch
           y_hat = self(x)
           loss = self.criterion(y_hat, y.unsqueeze(1))
           self.log('train_loss', loss)
           return loss

       def configure_optimizers(self):
           return optim.Adam(self.parameters(), lr=0.001)
   ```

2. **Instantiate Data Module and Logger:**
   ```python
   from pytorch_lightning.loggers import CSVLogger

   data_module = ChurnDataModule(batch_size=32)
   logger = CSVLogger("logs", name="churn_model")
   ```

### 8. Training Process

**Objective:** Train the model using PyTorch Lightning.

**Steps:**

1. **Instantiate Trainer:**
   ```python
   from pytorch_lightning import Trainer

   model = ChurnModel(input_dim=data_module.dataset.tensors[0].shape[1])
   trainer = Trainer(max_epochs=20, logger=logger, accelerator='cpu')
   ```

2. **Start Training:**
   ```python
   trainer.fit(model, datamodule=data_module)
   ```

### 9. Metrics Logging

**Objective:** Log and visualize training metrics.

**Steps:**

1. **Log Metrics:**
   - Metrics such as training and validation loss, accuracy, and F1 score are logged during training.

2. **Visualize Metrics:**
   ```python
   metrics = pd.read_csv(f"{logger.log_dir}/metrics.csv")
   sns.lineplot(data=metrics, x='epoch', y='train_loss')
   plt.title('Training Loss Over Epochs')
   plt.show()
   ```

### 10. Evaluating Metrics

**Objective:** Evaluate the model's performance using various metrics.

**Steps:**

1. **Compute Metrics:**
   ```python
   from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

   val_outputs = model(data_module.dataset.tensors[0])
   y_pred = (val_outputs > 0.5).float()
   y_true = data_module.dataset.tensors[1]

   accuracy = accuracy_score(y_true, y_pred)
   precision = precision_score(y_true, y_pred)
   recall = recall_score(y_true, y_pred)
   f1 = f1_score(y_true, y_pred)

   print(f"Accuracy: {accuracy}")
   print(f"Precision: {precision}")
   print(f"Recall: {recall}")
   print(f"F1 Score: {f1}")
   ```

### 11. Predictions and Final Evaluation

**Objective:** Make predictions and evaluate the model's performance.

**Steps:**

1. **Make Predictions:**
   ```python
   predictions = model(data_module.dataset.tensors[0])
   probabilities = torch.sigmoid(predictions)
   ```

2. **Final Evaluation:**
   - Compute final evaluation metrics to assess the model's performance.

## Summary

This lesson plan covers the essential steps for setting up a machine learning environment, preprocessing data, building a neural network, and evaluating its performance using PyTorch Lightning. By following these steps, you can efficiently prepare and train a classification model on a churn modeling dataset.

---

**Note:** Ensure you have the `churn_data.csv` dataset available in your working directory to follow along with the code examples.
